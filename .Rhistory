res_list$list
# install packages from CRAN
p_needed <- c("plyr", "dplyr", "stringr", "lubridate", "jsonlite", "httr", "xml2", "rvest", "devtools", "ggmap",  "networkD3", "RSelenium", "pageviews", "aRxiv", "twitteR", "streamR")
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
if (length(p_to_install) > 0) {
install.packages(p_to_install)
}
lapply(p_needed, require, character.only = TRUE)
warnings()
source("00-course-setup.r")
wd <- getwd()
getwd()
tempwd <- ("data/breweriesChicago")
dir.create(tempwd)
setwd(tempwd)
getwd()
url <- "https://www.google.de/?#q=list+breweries+chicago"
browseURL(url)
url <- "http://thehopreview.com/blog/chicago-brewery-list"
content <- read_html(url, encoding = "utf8")
anchors <- html_nodes(content, css = "#block-yui_3_17_2_8_1438187725105_11398 p")
breweries <- html_text(anchors)
length(breweries)
head(breweries)
breweries <- breweries[-1]
head(breweries)
warnings()
warning()
locations <- str_extract(breweries, "[[:digit:]].+?–")
locations <- str_replace(locations, "–", ", Chicago, IL")
locations <- locations[!is.na(locations)]
if (!file.exists("breweries_geo.RData")){
pos <- geocode(locations, source = "google")
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom="auto", maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=3)
p
help("read_html")
help("html_nodes")
url <- "https://www.google.de/?#q=list+breweries+chicago"
browseURL(url)     ##rlg: accessing webpages--it pops up
head(content)
content
head(anchors)
help("html_text")
head(breweries)
if (!file.exists("breweries_geo.RData")){
pos <- geocode(locations, source = "google") ##rlg: geocode from ggmap package, querying google maps api
geocodeQueryCheck()
save(pos, file="breweries_geo.RData")
} else {
load("breweries_geo.RData")
}
head(pos)
geocodeQueryCheck()
help("get_map")
brewery_map <- get_map(location=c(lon=mean(pos$lon), lat=mean(pos$lat)), zoom="auto", maptype="hybrid")
p <- ggmap(brewery_map) + geom_point(data=pos, aes(x=lon, y=lat), col="red", size=2)
p
setwd(wd)
# set temporary working directory
tempwd <- ("data/wikipediaStatisticians")
dir.create(tempwd)
setwd(tempwd)
## step 1: inspect page
url <- "https://en.wikipedia.org/wiki/List_of_statisticians"
browseURL(url)
html <- read_html(url)
head(html)
html
anchors <- html_nodes(html, xpath="//a")
length(anchors) # probably too many?
anchors <- html_nodes(html, xpath="//ul/li/a[1]")
links <- html_attr(anchors, "href")
anchors
links
links_iffer <-
seq_along(links) >=
seq_along(links)[str_detect(links, "Odd_Aalen")] &
seq_along(links) <=
seq_along(links)[str_detect(links, "George_Kingsley_Zipf")] &
str_detect(links, "/wiki/")
links_index <- seq_along(links)[links_iffer]
links <- links[links_iffer]
length(links)
links
names <- html_attr(anchors, "title")[links_index]
names <- str_replace(names, " \\(.*\\)", "")
## step 4: fetch personal wiki pages
baseurl <- "http://en.wikipedia.org"
HTML <- list()
Fname <- str_c(basename(links), ".html")
URL <- str_c(baseurl, links)
# loop
for ( i in seq_along(links) ){
# url
url <- URL[i]
# fname
fname <- Fname[i]
# download
if ( !file.exists(fname) ) download.file(url, fname)
# read in files
HTML[[i]] <- read_html(fname)
}
head(fname)
fname
head(Fname)
head(HTML)
connections <- data.frame(from=NULL, to=NULL)
# loop
for (i in seq_along(HTML)) {
pslinks <- html_attr(
html_nodes(HTML[[i]], xpath="//p//a"), # note: only look for links in p sections; otherwise too many links collected
"href")
links_in_pslinks <- seq_along(links)[links %in% pslinks] ##rlg: loook to see if links match
links_in_pslinks <- links_in_pslinks[links_in_pslinks!=i]
connections <- rbind(
connections,
data.frame(
from=rep(i-1, length(links_in_pslinks)), # -1 for zero-indexing
to=links_in_pslinks-1 # here too
)
)
}
names(connections) <- c("from", "to")
head(connections)
connections <- rbind(
connections,
data.frame(from=connections$to,
to=connections$from)
)
## step 6: visualize connections
dir.create("figures")
connections$value <- 1
nodesDF <- data.frame(name = names, group = 1)
network_out <- forceNetwork(Links = connections, Nodes = nodesDF, Source = "from", Target = "to", Value = "value", NodeID = "name", Group = "group", zoom = TRUE, opacityNoHover = 3)
saveNetwork(network_out, file = 'connections.html')
browseURL("connections.html")
browseURL("https://www.buzzfeed.com/?country=us")
browseURL("http://flukeout.github.io/")
browseURL("https://www.jstatsoft.org/about/editorialTeam")
url <- "https://www.buzzfeed.com/?country=us"
url_parsed <- read_html(url)
class(url_parsed)
html_structure(url_parsed)
as_list(url_parsed)
html_structure(url_parsed)
as_list(url_parsed)
headings_nodes <- html_nodes(url_parsed, css = ".lede__link")
headings_
headings_nodes
headings <- html_text(headings_nodes)
headings
headings <- str_replace_all(headings, "\\n", "") %>% str_trim()
headings
url <- "https://www.jstatsoft.org/about/editorialTeam"
# 2. download static HTML behind the URL and parse it into an XML file
url_parsed <- read_html(url)
# 3. extract specific nodes with CSS (or XPath)
headings_nodes2 <- html_nodes(url_parsed, css = ".member a")
# 4. extract content from nodes
headings2 <- html_text(headings_nodes2)
headings2
#bonus:
# 1. specify URL
url <- "https://www.jstatsoft.org/about/editorialTeam"
# 2. download static HTML behind the URL and parse it into an XML file
url_parsed <- read_html(url)
# 3. extract specific nodes with CSS (or XPath)
headings_nodes3 <- html_nodes(url_parsed, css = ".member li")
# 4. extract content from nodes
headings3 <- html_text(headings_nodes3)
headings3
affilstats <- str_extract(headings3, "Statistics")
affilstats
affilmath <- str_extract(headings3, "Mathematics")
affilmath
length(affilstats)
length(!is.na(affilstats))
length(!is.na(affilstats))
count(!is.na(affilstats))
!is.na(affilstats)
affilstats[!is.na(affilstats)]
length(affilstats[!is.na(affilstats)])
length(affilmath[!is.na(affilstats)])
length(affilmath[!is.na(affilmath)])
affilpsych <- str_extract(headings3, "Psychology")
length(affilpsych[!is.na(affilpsych)])
affilpsych
affilAll <-affilstats+affilmath+affilpsych
affilAll <-c(affilstats, affilmath, affilpsych)
affilAll
length(affilstats[!is.na(affilstats)&!is.na(affilmath)])
library(mosaic)
cbind(affilstats, affilmath, affilpsych)
tallY(affilstats,affilmath)
data <- cbind(affilstats, affilmath, affilpsych)
tallY(affilstats~affilmath, data)
library(mosaic)
data <- cbind(affilstats, affilmath, affilpsych)
tallY(affilstats~affilmath, data)
tally(affilstats~affilmath, data)
tally(~affilmath, data)
dat <- cbind(affilstats, affilmath, affilpsych)
tally(~affilmath, dat)
table(affilstats~affilmath)
tab<-table(affilstats~affilmath)
tab
help("tally")
dat
tally(~affilmath|affilstats, dat)
mosaic::tally(~affilmath|affilstats, dat)
View(data)
rm(data)
mosaic::tally(~affilmath|affilstats, data=dat)
tally(~affilmath|affilstats, data=dat)
tally(~as.int(affilmath)|as.int(affilstats), data=dat)
tally(~as.factor(affilmath)|as.factor(affilstats), data=dat)
tally(~as.factor(affilmath)|as.factor(affilstats), data=dat, envir = parent.frame())
class(dat)
tally(~as.factor(affilmath)|as.factor(affilstats), data=as.dataframe(dat), envir = parent.frame())
tally(~as.factor(affilmath)|as.factor(affilstats), data=as.data.frame(as.table(dat)), envir = parent.frame())
url <- "https://en.wikipedia.org/wiki/Joint_Statistical_Meetings"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)
tables
meetings <- tables[[2]]
class(meetings)
head(meetings)
table(meetings$Location) %>% sort()
browseurl("https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world")
browseurl("https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world")
browseURL("https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world")
url <- "https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world"
url_parsed <- read_html(url)
tables <- html_table(url_parsed, fill = TRUE)##rlg: function html_table will find tables
tables
str(tables)
tables
head(tables)
tables[[8]]
tables[[7]]
tables[[9]]
tables[[3]]
tables[[2]]
head(tables[[2]])
head(tables[[3]])
head(tables[[7]])
browseURL("https://en.wikipedia.org/wiki/List_of_tallest_buildings_in_the_world")
table(buildings$Country/region) %>% sort()
buildings <- tables[[7]]
class(buildings)
head(buildings)
table(buildings$Country/region) %>% sort()
vignette("selectorgadget")
vignette("selectorgadget")
browseURL("http://selectorgadget.com/")
browseURL("https://www.buzzfeed.com/?country=us")
browseURL("https://www.buzzfeed.com/?country=us")
browseURL("https://www.java.com/en/download/")
# install current version of Firefox browser
browseURL("https://www.mozilla.org/en-US/firefox/new/")
# install dev version of RSelenium
devtools::install_github("ropensci/RSelenium")
tempwd <- ("data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
browseURL("http://www.jstatsoft.org/")
# construct list of urls
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,71,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
urls_list
names
folder <- "html_articles/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, "/", names[i]))
Sys.sleep(runif(1, 0, 1))
}
}
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
files_size <- sapply(list_files_path, file.size)
table(files_size)
delete_files <- list_files_path[files_size == 22094]
sapply(delete_files, file.remove)
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE) # update list of files
# check success
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
tempwd <- ("data/jstatsoftStats")
dir.create(tempwd)
setwd(tempwd)
baseurl <- "http://www.jstatsoft.org/article/view/v"
volurl <- paste0("0", seq(1,71,1))
volurl[1:9] <- paste0("00", seq(1, 9, 1))
brurl <- paste0("0", seq(1,9,1))
urls_list <- paste0(baseurl, volurl)
urls_list <- paste0(rep(urls_list, each = 9), "i", brurl)
names <- paste0(rep(volurl, each = 9), "_", brurl, ".html")
# download pages
folder <- "html_articles/"
dir.create(folder)
for (i in 1:length(urls_list)) {
if (!file.exists(paste0(folder, names[i]))) {
download.file(urls_list[i], destfile = paste0(folder, "/", names[i]))
Sys.sleep(runif(1, 0, 1))##rlg:taking pauses between downloads to fly under the radar
}
}
list_files <- list.files(folder, pattern = "0.*")
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE)
length(list_files)
files_size <- sapply(list_files_path, file.size)
table(files_size)
delete_files <- list_files_path[files_size == 22094]
sapply(delete_files, file.remove)
list_files_path <-  list.files(folder, pattern = "0.*", full.names = TRUE) # update list of files
length(list_files)
length(list_files_path)
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
library(rvest)
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
# install packages from CRAN
p_needed <- c("plyr", "dplyr", "stringr", "lubridate", "jsonlite", "httr", "xml2", "rvest", "devtools", "ggmap",  "networkD3", "RSelenium", "pageviews", "aRxiv", "twitteR", "streamR")
packages <- rownames(installed.packages())
p_to_install <- p_needed[!(p_needed %in% packages)]
if (length(p_to_install) > 0) {
install.packages(p_to_install)
}
lapply(p_needed, require, character.only = TRUE)
# import pages and extract content
authors <- character()
title <- character()
statistics <- character()
numViews <- numeric()
datePublish <- character()
for (i in 1:length(list_files_path)) {
html_out <- read_html(list_files_path[i])
table_out <- html_table(html_out, fill = TRUE)[[6]]
authors[i] <- table_out[1,2]
title[i] <- table_out[2,2]
statistics[i] <- table_out[4,2]
numViews[i] <- statistics[i] %>% str_extract("[[:digit:]]+") %>% as.numeric()
datePublish[i] <- statistics[i] %>% str_extract("[[:digit:]]{4}-[[:digit:]]{2}-[[:digit:]]{2}.$") %>% str_replace("\\.", "")
}
dat <- data.frame(authors = authors, title = title, numViews = numViews, datePublish = datePublish)
head(dat)
dattop <- dat[order(dat$numViews, decreasing = TRUE),]
dattop[1:10,]
summary(dat$numViews)
plot(density(dat$numViews), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JSTATSOFT")
plot(density(!is.na(dat$numViews)), yaxt="n", ylab="", xlab="Number of views", main="Distribution of article page views in JSTATSOFT")
devtools::install_github("ropensci/RSelenium")
browseURL("https://www.java.com/en/download/")
browseURL("https://www.mozilla.org/en-US/firefox/new/")
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
browseURL(url)
content <- read_html(url)
checkForServer()
startServer()
startServer()
source("00-course-setup.r")
wd <- getwd()
devtools::install_github("ropensci/RSelenium")
checkForServer()
startServer()
remDr <- remoteDriver(remoteServerAddr = "localhost", port = 4444, browserName = "firefox")
remDr$open()
remDr$navigate(url)
url <- "http://www.iea.org/policiesandmeasures/renewableenergy/"
content <- read_html(url)
remDr$navigate(url)
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement()
css <- 'div.form-container:nth-child(6) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
clickFrom <- fromDrop$clickElement()
writeFrom <- fromDrop$sendKeysToElement(list("2000"))
css <- 'div.form-container:nth-child(6) > select:nth-child(2)'
fromDrop <- remDr$findElement(using = 'css', value = css)
remDr$navigate(url)
# open regions menu
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement() # click on button
# selection "European Union"
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > ul:nth-child(3) > li:nth-child(5) > label:nth-child(1) > input:nth-child(1)'
euElem <- remDr$findElement(using = 'css', value = css)
selectEU <- euElem$clickElement() # click on button
remDr$navigate(url)
# open regions menu
css <- 'div.form-container:nth-child(2) > ul:nth-child(2) > li:nth-child(1) > span:nth-child(1)'
regionsElem <- remDr$findElement(using = 'css', value = css)
openRegions <- regionsElem$clickElement()
browseURL("https://github.com/ropensci/opendata")
browseURL("http://export.arxiv.org/api/query?search_query=all:forecast")
forecast <- read_xml("http://export.arxiv.org/api/query?search_query=all:forecast")
xml_ns(forecast) # inspect namespaces
authors <- xml_find_all(forecast, "//d1:author", ns = xml_ns(forecast))
authors %>% xml_text()
library(aRxiv)
ls("package:aRxiv")
?arxiv_search
arxiv_df <- arxiv_search(query = "forecast AND submittedDate:[2016 TO 2017]", limit = 1000, output_format = "data.frame")
View(arxiv_df)
arxiv_count('au:"Gary King"')
query_terms
arxiv_count('abs:"political" AND submittedDate:[2016 TO 2017]')
polsci_articles <- arxiv_search('abs:"political" AND submittedDate:[2016 TO 2017]', limit = 1000)
View(polsci_articles)
ls("package:pageviews")
help("pageviews")
?article_pageviews
obama_pageviews <- article_pageviews(article = "Barack_Obama")
obama_pageviews
obama_pageviews <- article_pageviews(article = "Barack Obama")
obama_pageviews
clinton_pageviews <- article_pageviews(article = "Hillary Clinton")
trump_pageviews <- article_pageviews(article = "Donald Trump")
clinton_pageviews
trump_pageviews
ls("package:pageviews")
?project_pageviews
clinton_pageviews <- article_pageviews(article = "Hillary_Clinton")
trump_pageviews <- article_pageviews(article = "Donald_Trump")
clinton_pageviews
trump_pageviews
ls("package:pageviews")
?article_pageviews
?article_pageviews
?project_pageviews
?article_pageviews
?project_pageviews
Head(clinton_pageviews)
head(clinton_pageviews)
trump_views <- article_pageviews(project = "en.wikipedia", article = "Donald Trump", user_type = "user", start = "2016010100", end = "20160720")
head(trump_views)
clinton_views <- article_pageviews(project = "en.wikipedia", article = "Hillary Clinton", user_type = "user", start = "2016010100", end = "20160720")
plot(ymd_h(trump_views$timestamp), trump_views$views, col = "red", type = "l")
lines(ymd_h(clinton_views$timestamp), clinton_views$views, col = "blue")
title <- "Groundhog Day" %>% URLencode()
endpoint <- "http://www.omdbapi.com/?"
url <- paste0(endpoint, "t=", title, "&tomatoes=true")
url
omdb_res = GET(url)
omdb_res
res_list <- content(omdb_res, as =  "parsed")
res_list %>% unlist() %>% t() %>% data.frame(stringsAsFactors = FALSE)
view(res_list)
url <- paste0(endpoint, "s=", title)
omdb_res = GET(url)
res_list <- content(omdb_res, as = "text") %>% jsonlite::fromJSON(flatten = TRUE)
res_list$Search
browseURL("http://openweathermap.org/current")
browseURL("http://openweathermap.org/api")
city <- "Chicago" %>% URLencode()
endpoint <- "api.openweathermap.org/data/2.5/weather?"
url <- paste0(endpoint, "q=", city)
weather_res = GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list
city <- "Chicago" %>% URLencode()
endpoint <- "api.openweathermap.org/data/2.5/weather?"
url <- paste0(endpoint, "q=", city, "&APPID=8c75432807e6ac3cf1ce8211c360b5ca")
weather_res = GET(url)
res_list <- content(weather_res, as =  "parsed")
res_list
